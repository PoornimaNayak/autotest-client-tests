Package Name: perl-WWW-RobotRules

Summary     : WWW::RobotRules - database of robots.txt-derived permissions


Description :

This module parses /robots.txt files as specified in "A Standard for Robot
Exclusion", at <http://www.robotstxt.org/wc/norobots.html> Webmasters can use
the /robots.txt file to forbid conforming robots from accessing parts of their
web site.

The parsed files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given URL is prohibited. The same
WWW::RobotRules object can be used for one or more parsed /robots.txt files on
any number of hosts.

The following methods are provided:

$rules = WWW::RobotRules->new($robot_name)

    This is the constructor for WWW::RobotRules objects. The first argument
given to new() is the name of the robot.
$rules->parse($robot_txt_url, $content, $fresh_until)

    The parse() method takes as arguments the URL that was used to retrieve
the /robots.txt file, and the contents of the file.
$rules->allowed($uri)

    Returns TRUE if this robot is allowed to retrieve this URL.
$rules->agent([$name])

    Get/set the agent name. NOTE: Changing the agent name will clear the
robots.txt rules and expire times out of the cache.


Source :
http://search.cpan.org/~gaas/WWW-RobotRules-6.02/lib/WWW/RobotRules.pm


To run all tests:
cd /opt/fiv/ltp/testcases/fivextra/perl-WWW-RobotRules
run the script
./perl-WWW-RobotRules.sh

To run test individually:
cd /opt/fiv/ltp/testcases/fivextra/perl-WWW-RobotRules/t
perl t/<test.t>
